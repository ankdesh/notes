## Literature survey for neural network generation using Deep Reinforment Learning


| Title                                                                                   | Summary                                                                                                                                                                                                                                                                                                                                                                  | Link                             | Affiliations      |
|-----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|-------------------|
| Designing Neural Network Architectures using Reinforcement Learning                     | The learning agent is trained to sequentially choose CNN layers using Q-learning with an Ïµ-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task.                                                                    | https://arxiv.org/abs/1611.02167 | MIT               |
| Neural Architecture Search with Reinforcement Learning                                  | In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.                                                                                                                                     | https://arxiv.org/abs/1611.01578 | Google            |
| Learning Transferable Architectures for Scalable Image Recognition                      | The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability.                                                                                                                                                                                                                                         | https://arxiv.org/abs/1707.07012 | Google Brain      |
| Efficient Neural Architecture Search via Parameter Sharing                              | a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set.                                                                                                      | https://arxiv.org/abs/1802.03268 | Google Brain      |
| N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning | Our approach takes a larger `teacher' network as input and outputs a compressed `student' network derived from the `teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large `teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. | https://arxiv.org/abs/1709.06030 | CMU               |
| Resource-Efficient Neural Architect                                                     | RENA uses a policy network to process the network embeddings to generate new configurations. We demonstrate RENA on image recognition and keyword spotting (KWS) problems.                                                                                                                                                                                               | https://arxiv.org/abs/1806.07912 | Baidu             |
| SMASH: One-Shot Model Architecture Search through HyperNetworks                         | accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture                                                                                                                                                                                                                  | https://arxiv.org/abs/1708.05344 | Edinburgh         |
| Efficient Architecture Search by Network Transformation                                 | a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights.                                                                                                                                                                                                                           | https://arxiv.org/abs/1707.04873 | London University |
| DARTS: Differentiable Architecture Search                                               | method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent.                                                                                                                                                                                                                   | https://arxiv.org/abs/1806.09055 | CMU               |
| ADC: Automated Deep Compression and Acceleration with Reinforcement Learning            | leverages reinforcement learning in order to efficiently sample the design space and greatly improve the model compression quality.                                                                                                                                                                                                                                      | https://arxiv.org/abs/1802.03494 | SongHan, MIT      |
